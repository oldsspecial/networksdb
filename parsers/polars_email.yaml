pipeline:
  name: polars_dedup
  description: "Demonstrates high-performance deduplication using Polars DataFrames with schema-defined merge strategies"
  schema_modules: ["networksdb"]

  stages:
    # Stage 1: Read data from Parquet (high throughput)
    - id: "read_network_data"
      type: "polars_reader"
      config:
        path: "data/email_data.csv"
        batch_size: 100000  # Process 100K records per chunk

    # Stage 2: Parse raw data into entities
    - id: "parse_entities"
      type: "polars_parser"
      input: "read_network_data"
      config:
        mapping:
          imports:
            - parsers/email_mapping.yaml
        batch_size: 100000

    # Stage 3: Deduplicate entities with Polars (50K-100K entities/sec)
    - id: "deduplicate"
      type: "polars_dedup"
      input: "parse_entities"
      config:
        # Enable lazy execution for query optimization (default: true)
        use_lazy: true

        # Skip dynamic properties merging for performance (default: false)
        # skip_dynamic_properties: false

        # Optional: Override merge strategies per entity type
        # merge_strategy_overrides:
        #   IPAddress:
        #     hostname: "take_last"        # Keep most recent hostname
        #     vlan_id: "max"               # Keep highest VLAN ID
        #     status: "take_any_non_null"  # Prefer non-null values
        #   Connected:
        #     bandwidth_mbps: "max"        # Keep highest bandwidth

    # Stage 4: Write deduplicated data to Parquet
    - id: "write_output"
      type: "jsonl_writer"
      input: "deduplicate"
      config:
        nodes_path: "output/nodes.jsonl"
        relationships_path: "output/relationships.jsonl"
#        compression: "zstd"

# ---
# Performance characteristics:
#   - Throughput: 50K-100K entities/sec
#   - Memory: 2-3x input size (efficient DataFrame operations)
#   - Scaling: Near-linear from 10K to 10M entities
#
# Merge strategies (defined in schema, overridable here):
#   - error_if_different: Fail if duplicates have different values
#   - take_first: Keep first occurrence
#   - take_last: Keep last occurrence
#   - take_any_non_null: Prefer non-null values
#   - min/max: Numeric minimum/maximum
#   - sum: Numeric sum (for counters, metrics)
#   - union: Combine unique values from lists
#
# Use cases:
#   - Network log deduplication (100K+ events)
#   - Multi-source data reconciliation
#   - Time-series event consolidation
#   - Large-scale entity resolution
